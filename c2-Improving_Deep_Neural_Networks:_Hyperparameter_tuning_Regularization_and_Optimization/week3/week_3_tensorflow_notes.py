# -*- coding: utf-8 -*-
"""Week 3 TensorFlow notes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Flxkih62hJm1WgpYnBlhe3C_QdwZ2P5j

# Week 3 - TensorFlow

This colab jupyter notebook contains notes from week 3's TensorFlow video.

Welcome to the last video for this week. There are many great, deep learning programming frameworks. One of them is TensorFlow.

I'm excited to help you start to learn to use TensorFlow. What I want to do in this video is show you the basic structure of a TensorFlow program, and then leave you to practice, learn more details, and practice them yourself in this week's problem exercise. This week's problem exercise will take some time to do so please be sure to leave some extra time to do it.

As a motivating problem, let's say that you have some cost function J that you want to minimize. And for this example, I'm going to use this highly simple cost function **J(w) = w squared- 10w + 25**. So that's the cost function.

**Let's get started**
"""

import numpy as np
import tensorflow as tf

# initialize w to 0, w is the parameter we're trying to optimize
w = tf.Variable(0, dtype=tf.float32)
# define a cost function, w**2 is an element wise power
# tensorflow already knows how to perform backprop because it's built-in multuply and **
cost = tf.add(tf.add(w**2, tf.multiply(-10., w)), 25)
# learning algorithm
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)

init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)
print(session.run(w))

session.run(train)
print(session.run(w))

# run a 1000 iterations
for i in range(1000):
  session.run(train)
print(session.run(w))

"""**nicer syntax**

The code below is the same code as above but with nicer syntax
"""

# initialize w to 0, w is the parameter we're trying to optimize
w = tf.Variable(0, dtype=tf.float32)
# define a cost function, w**2 is an element wise power
# tensorflow already knows how to perform backprop because it's built-in multuply and **
cost = w**2 - 10*w + 25
# learning algorithm
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)

init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)
print(session.run(w))

"""**Same Example, with x training data**"""

import numpy as np
import tensorflow as tf

# data that will be inserted in x
coefficients = np.array([[1.], [-10.], [25.]])

# initialize w to 0, w is the parameter we're trying to optimize
w = tf.Variable(0, dtype=tf.float32)
# tells tensor flow that you will provide values later
x = tf.placeholder(tf.float32, [3, 1])
# define a cost function, w**2 is an element wise power
# tensorflow already knows how to perform backprop because it's built-in multuply and **
cost = w**2 - 10*w + 25
# learning algorithm
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)

init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)

session.run(train, feed_dict={x:coefficients})
print('W after 1 iteration: ' + str(session.run(w)))

# run a 1000 iterations
for i in range(1000):
  session.run(train, feed_dict={x:coefficients})
print('W after a 1000 iterations: ' + str(session.run(w)))

"""**Same Example, with different coefficients**"""

import numpy as np
import tensorflow as tf

# data that will be inserted in x
coefficients = np.array([[1.], [-20.], [100.]])

# initialize w to 0, w is the parameter we're trying to optimize
w = tf.Variable(0, dtype=tf.float32)
# tells tensor flow that you will provide values later
x = tf.placeholder(tf.float32, [3, 1])
# define a cost function, w**2 is an element wise power
# tensorflow already knows how to perform backprop because it's built-in multuply and **
cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]
# learning algorithm, can use adam too
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)

init = tf.global_variables_initializer()

## Next 3 lines can be used another way, see next code cell
session = tf.Session()
session.run(init)
# feed_dict can be used with batch gradien descent to feed in subset of data
session.run(train, feed_dict={x:coefficients})


print('W after 1 iteration: ' + str(session.run(w)))

# run a 1000 iterations
for i in range(1000):
  session.run(train, feed_dict={x:coefficients})
print('W after a 1000 iterations: ' + str(session.run(w)))

"""**Yet another way to code the example**"""

import numpy as np
import tensorflow as tf

# data that will be inserted in x
coefficients = np.array([[1.], [-20.], [100.]])

# initialize w to 0, w is the parameter we're trying to optimize
w = tf.Variable(0, dtype=tf.float32)
# tells tensor flow that you will provide values later
x = tf.placeholder(tf.float32, [3, 1])
# define a cost function, w**2 is an element wise power
# tensorflow already knows how to perform backprop because it's built-in multuply and **
cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]
# learning algorithm, can use adam too
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)

init = tf.global_variables_initializer()

with tf.Session() as session:
  session.run(init)
  session.run(train, feed_dict={x:coefficients})

  print('W after 1 iteration: ' + str(session.run(w)))

  # run a 1000 iterations
  for i in range(1000):
    session.run(train, feed_dict={x:coefficients})
  print('W after a 1000 iterations: ' + str(session.run(w)))

